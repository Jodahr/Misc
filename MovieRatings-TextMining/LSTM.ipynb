{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Movie Reviews - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import nltk\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from operator import itemgetter\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Was geht ab hier drüben ärgerte?',\n",
       " 'Waß soll das hier',\n",
       " 'Hier ist es total langweilig']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"Was geht ab hier drüben ärgerte?\", \"Waß soll das hier\", \"Hier ist es total langweilig\"]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = utils.CorpusEncoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['was', 'geht', 'ab', 'hier', 'drueb', 'aergert'],\n",
       " ['wass', 'soll', 'das', 'hier'],\n",
       " ['hier', 'ist', 'es', 'total', 'langweil']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tok = [utils.text_preprocess(doc, stop=False, language='german') for doc in corpus]\n",
    "corpus_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce.fit(corpus_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ce.reduce_vocab(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 14, 9, 4, 3, 7, 5], [1, 15, 12, 6, 3], [1, 3, 10, 8, 13, 11]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce.transform(corpus_tok, drop_unknown=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcount_encoding(corpus, upper=None, lower=None):\n",
    "    corpus_prep = []\n",
    "    for doc in corpus:\n",
    "        doc = utils.rem_html(doc)\n",
    "        doc = utils.rem_punctuation(doc)\n",
    "        doc = utils.rem_additional_whitespaces(doc)\n",
    "        doc = nltk.word_tokenize(doc)\n",
    "        doc = [element.lower() for element in doc]\n",
    "        #doc = utils.stem_words(doc, language='english')\n",
    "        corpus_prep.append(doc)\n",
    "    words, counts = utils.wordcount_corpus(corpus_prep)\n",
    "    sorted_tuplelist = np.array(sorted(zip(words,counts), key=itemgetter(1), reverse=True))\n",
    "    sorted_tuplelist = sorted_tuplelist[upper:lower,:]\n",
    "    print(sorted_tuplelist)\n",
    "    vocab_size = len(sorted_tuplelist)\n",
    "    indices = np.arange(vocab_size) + 3\n",
    "    word_to_index = dict(zip(sorted_tuplelist[:,0], indices))\n",
    "    #word_to_index = {key: value for key, value in wor}\n",
    "    word_to_index['<PAD>'] = 0\n",
    "    word_to_index['<START>'] = 1\n",
    "    word_to_index['<UNKNOWN>'] = 2\n",
    "    index_to_word = {value: key for key, value in word_to_index.items()}\n",
    "    corpus_enc = []\n",
    "    for doc in corpus_prep:\n",
    "        doc_prep = [1]\n",
    "        for word in doc:\n",
    "            try:\n",
    "                doc_prep.append(word_to_index[word])\n",
    "            except:\n",
    "                continue\n",
    "        corpus_enc.append(doc_prep)\n",
    "    return word_to_index, index_to_word, corpus_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hier' '3']\n",
      " ['ab' '1']\n",
      " ['das' '1']]\n"
     ]
    }
   ],
   "source": [
    "word_to_index, index_to_word, corps = wordcount_encoding(corpus, 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 'hier', 4: 'ab', 5: 'das', 0: '<PAD>', 1: '<START>', 2: '<UNKNOWN>'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index\n",
    "index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4, 3],\n",
       "       [1, 5, 3],\n",
       "       [0, 1, 3]], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(corps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 3 columns):\n",
      "id           25000 non-null object\n",
      "sentiment    25000 non-null int64\n",
      "review       25000 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 586.0+ KB\n",
      "       id  sentiment                                             review\n",
      "0  5814_8          1  With all this stuff going down at the moment w...\n",
      "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
      "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
      "3  3630_4          0  It must be assumed that those who praised this...\n",
      "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.50001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment\n",
       "count  25000.00000\n",
       "mean       0.50000\n",
       "std        0.50001\n",
       "min        0.00000\n",
       "25%        0.00000\n",
       "50%        0.50000\n",
       "75%        1.00000\n",
       "max        1.00000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('labeledTrainData.tsv', sep='\\t')\n",
    "df.info()\n",
    "print(df.head())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "X_train, X_test = df_train['review'].values, df_test['review']\n",
    "y_train, y_test = df_train['sentiment'], df_test['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prep = [utils.text_preprocess(x) for x in X_train[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce.fit(X_train_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movi': 3,\n",
       " 'film': 4,\n",
       " 'one': 5,\n",
       " 'get': 6,\n",
       " 'time': 7,\n",
       " 'watch': 8,\n",
       " 'bad': 9,\n",
       " 'see': 10,\n",
       " 'stori': 11,\n",
       " 'thing': 12,\n",
       " 'would': 13,\n",
       " 'like': 14,\n",
       " 'peopl': 15,\n",
       " 'good': 16,\n",
       " 'much': 17,\n",
       " 'veri': 18,\n",
       " 'well': 19,\n",
       " 'ever': 20,\n",
       " 'got': 21,\n",
       " 'look': 22,\n",
       " 'onli': 23,\n",
       " 'read': 24,\n",
       " 'star': 25,\n",
       " 'think': 26,\n",
       " '10': 27,\n",
       " 'ani': 28,\n",
       " 'famili': 29,\n",
       " 'feel': 30,\n",
       " 'great': 31,\n",
       " 'made': 32,\n",
       " 'make': 33,\n",
       " 'need': 34,\n",
       " 'seen': 35,\n",
       " 'year': 36,\n",
       " 'act': 37,\n",
       " 'actual': 38,\n",
       " 'also': 39,\n",
       " 'back': 40,\n",
       " 'best': 41,\n",
       " 'camera': 42,\n",
       " 'charact': 43,\n",
       " 'everi': 44,\n",
       " 'gandhi': 45,\n",
       " 'go': 46,\n",
       " 'left': 47,\n",
       " 'life': 48,\n",
       " 'light': 49,\n",
       " 'origin': 50,\n",
       " 'perfect': 51,\n",
       " 'perform': 52,\n",
       " 'play': 53,\n",
       " 'seem': 54,\n",
       " 'shot': 55,\n",
       " 'show': 56,\n",
       " 'worst': 57,\n",
       " 'zombi': 58,\n",
       " '90': 59,\n",
       " 'actor': 60,\n",
       " 'beauti': 61,\n",
       " 'becaus': 62,\n",
       " 'bit': 63,\n",
       " 'book': 64,\n",
       " 'come': 65,\n",
       " 'day': 66,\n",
       " 'director': 67,\n",
       " 'even': 68,\n",
       " 'figur': 69,\n",
       " 'gore': 70,\n",
       " 'guy': 71,\n",
       " 'hammer': 72,\n",
       " 'happen': 73,\n",
       " 'horror': 74,\n",
       " 'know': 75,\n",
       " 'main': 76,\n",
       " 'might': 77,\n",
       " 'move': 78,\n",
       " 'new': 79,\n",
       " 'noth': 80,\n",
       " 'particular': 81,\n",
       " 'point': 82,\n",
       " 'realli': 83,\n",
       " 'set': 84,\n",
       " 'sinc': 85,\n",
       " 'two': 86,\n",
       " 'whi': 87,\n",
       " 'wonder': 88,\n",
       " 'work': 89,\n",
       " 'young': 90,\n",
       " 'action': 91,\n",
       " 'appear': 92,\n",
       " 'comedi': 93,\n",
       " 'could': 94,\n",
       " 'end': 95,\n",
       " 'enjoy': 96,\n",
       " 'entertain': 97,\n",
       " 'experi': 98,\n",
       " 'extrem': 99,\n",
       " 'fan': 100,\n",
       " 'final': 101,\n",
       " 'friend': 102,\n",
       " 'god': 103,\n",
       " 'help': 104,\n",
       " 'idea': 105,\n",
       " 'inspir': 106,\n",
       " 'interest': 107,\n",
       " 'level': 108,\n",
       " 'lot': 109,\n",
       " 'love': 110,\n",
       " 'man': 111,\n",
       " 'mani': 112,\n",
       " 'mean': 113,\n",
       " 'minut': 114,\n",
       " 'moment': 115,\n",
       " 'offens': 116,\n",
       " 'old': 117,\n",
       " 'person': 118,\n",
       " 'pestil': 119,\n",
       " 'problem': 120,\n",
       " 'put': 121,\n",
       " 'rememb': 122,\n",
       " 'scene': 123,\n",
       " 'someon': 124,\n",
       " 'someth': 125,\n",
       " 'son': 126,\n",
       " 'spiritu': 127,\n",
       " 'still': 128,\n",
       " 'symbol': 129,\n",
       " 'take': 130,\n",
       " 'tell': 131,\n",
       " 'tri': 132,\n",
       " 'true': 133,\n",
       " 'understand': 134,\n",
       " 'wait': 135,\n",
       " 'war': 136,\n",
       " 'wast': 137,\n",
       " 'way': 138,\n",
       " 'whole': 139,\n",
       " 'wors': 140,\n",
       " 'worth': 141,\n",
       " '2000': 142,\n",
       " '4': 143,\n",
       " 'alon': 144,\n",
       " 'although': 145,\n",
       " 'alway': 146,\n",
       " 'anoth': 147,\n",
       " 'anyway': 148,\n",
       " 'apart': 149,\n",
       " 'art': 150,\n",
       " 'ask': 151,\n",
       " 'attack': 152,\n",
       " 'audienc': 153,\n",
       " 'away': 154,\n",
       " 'begin': 155,\n",
       " 'beowulf': 156,\n",
       " 'bore': 157,\n",
       " 'boy': 158,\n",
       " 'break': 159,\n",
       " 'brilliant': 160,\n",
       " 'cast': 161,\n",
       " 'classic': 162,\n",
       " 'clock': 163,\n",
       " 'confus': 164,\n",
       " 'convers': 165,\n",
       " 'craft': 166,\n",
       " 'creat': 167,\n",
       " 'dahmer': 168,\n",
       " 'direct': 169,\n",
       " 'doe': 170,\n",
       " 'done': 171,\n",
       " 'dream': 172,\n",
       " 'dumb': 173,\n",
       " 'entir': 174,\n",
       " 'excel': 175,\n",
       " 'far': 176,\n",
       " 'father': 177,\n",
       " 'favorit': 178,\n",
       " 'find': 179,\n",
       " 'first': 180,\n",
       " 'flick': 181,\n",
       " 'full': 182,\n",
       " 'gift': 183,\n",
       " 'greatest': 184,\n",
       " 'hero': 185,\n",
       " 'high': 186,\n",
       " 'hous': 187,\n",
       " 'incred': 188,\n",
       " 'last': 189,\n",
       " 'line': 190,\n",
       " 'live': 191,\n",
       " 'mahatma': 192,\n",
       " 'modern': 193,\n",
       " 'murphi': 194,\n",
       " 'music': 195,\n",
       " 'must': 196,\n",
       " 'next': 197,\n",
       " 'nice': 198,\n",
       " 'part': 199,\n",
       " 'plot': 200,\n",
       " 'post': 201,\n",
       " 'rather': 202,\n",
       " 'reason': 203,\n",
       " 'right': 204,\n",
       " 'robbin': 205,\n",
       " 'run': 206,\n",
       " 'sad': 207,\n",
       " 'schnaa': 208,\n",
       " 'second': 209,\n",
       " 'sens': 210,\n",
       " 'serious': 211,\n",
       " 'stuff': 212,\n",
       " 'sure': 213,\n",
       " 'tom': 214,\n",
       " 'top': 215,\n",
       " 'turn': 216,\n",
       " 'us': 217,\n",
       " 'want': 218,\n",
       " '1': 219,\n",
       " '3': 220,\n",
       " '40': 221,\n",
       " '5': 222,\n",
       " '7': 223,\n",
       " 'abl': 224,\n",
       " 'achiev': 225,\n",
       " 'ago': 226,\n",
       " 'amus': 227,\n",
       " 'anakin': 228,\n",
       " 'andrea': 229,\n",
       " 'angl': 230,\n",
       " 'approach': 231,\n",
       " 'ban': 232,\n",
       " 'beer': 233,\n",
       " 'believ': 234,\n",
       " 'ben': 235,\n",
       " 'better': 236,\n",
       " 'big': 237,\n",
       " 'bottom': 238,\n",
       " 'branch': 239,\n",
       " 'brief': 240,\n",
       " 'brown': 241,\n",
       " 'buy': 242,\n",
       " 'call': 243,\n",
       " 'challeng': 244,\n",
       " 'cheap': 245,\n",
       " 'child': 246,\n",
       " 'christi': 247,\n",
       " 'cinematographi': 248,\n",
       " 'combin': 249,\n",
       " 'common': 250,\n",
       " 'complet': 251,\n",
       " 'condit': 252,\n",
       " 'connect': 253,\n",
       " 'conrad': 254,\n",
       " 'coupl': 255,\n",
       " 'crap': 256,\n",
       " 'crowd': 257,\n",
       " 'danc': 258,\n",
       " 'dead': 259,\n",
       " 'deep': 260,\n",
       " 'deserv': 261,\n",
       " 'design': 262,\n",
       " 'detect': 263,\n",
       " 'dialogu': 264,\n",
       " 'doubt': 265,\n",
       " 'due': 266,\n",
       " 'dvd': 267,\n",
       " 'earth': 268,\n",
       " 'endear': 269,\n",
       " 'enough': 270,\n",
       " 'escap': 271,\n",
       " 'everyth': 272,\n",
       " 'exampl': 273,\n",
       " 'except': 274,\n",
       " 'exercis': 275,\n",
       " 'expect': 276,\n",
       " 'explain': 277,\n",
       " 'fair': 278,\n",
       " 'finish': 279,\n",
       " 'follow': 280,\n",
       " 'foot': 281,\n",
       " 'fulli': 282,\n",
       " 'funni': 283,\n",
       " 'futur': 284,\n",
       " 'genius': 285,\n",
       " 'georg': 286,\n",
       " 'girl': 287,\n",
       " 'give': 288,\n",
       " 'grade': 289,\n",
       " 'grant': 290,\n",
       " 'head': 291,\n",
       " 'heart': 292,\n",
       " 'hope': 293,\n",
       " 'hors': 294,\n",
       " 'howev': 295,\n",
       " 'human': 296,\n",
       " 'humor': 297,\n",
       " 'imag': 298,\n",
       " 'imagin': 299,\n",
       " 'interpret': 300,\n",
       " 'jackson': 301,\n",
       " 'job': 302,\n",
       " 'joke': 303,\n",
       " 'judg': 304,\n",
       " 'jump': 305,\n",
       " 'kid': 306,\n",
       " 'killer': 307,\n",
       " 'kinkad': 308,\n",
       " 'lawyer': 309,\n",
       " 'learn': 310,\n",
       " 'least': 311,\n",
       " 'lewi': 312,\n",
       " 'list': 313,\n",
       " 'local': 314,\n",
       " 'long': 315,\n",
       " 'low': 316,\n",
       " 'lower': 317,\n",
       " 'mad': 318,\n",
       " 'manner': 319,\n",
       " 'masterpiec': 320,\n",
       " 'may': 321,\n",
       " 'mend': 322,\n",
       " 'mike': 323,\n",
       " 'mind': 324,\n",
       " 'miss': 325,\n",
       " 'name': 326,\n",
       " 'nation': 327,\n",
       " 'nerv': 328,\n",
       " 'never': 329,\n",
       " 'newman': 330,\n",
       " 'open': 331,\n",
       " 'opinion': 332,\n",
       " 'oscar': 333,\n",
       " 'pace': 334,\n",
       " 'peter': 335,\n",
       " 'pinto': 336,\n",
       " 'pleas': 337,\n",
       " 'poem': 338,\n",
       " 'ponder': 339,\n",
       " 'popcorn': 340,\n",
       " 'premis': 341,\n",
       " 'pretend': 342,\n",
       " 'pride': 343,\n",
       " 'probabl': 344,\n",
       " 'pull': 345,\n",
       " 'push': 346,\n",
       " 'qualiti': 347,\n",
       " 'quit': 348,\n",
       " 'rabid': 349,\n",
       " 'real': 350,\n",
       " 'realiz': 351,\n",
       " 'recent': 352,\n",
       " 'relationship': 353,\n",
       " 'releas': 354,\n",
       " 'rest': 355,\n",
       " 'robert': 356,\n",
       " 'roeg': 357,\n",
       " 'ron': 358,\n",
       " 'russian': 359,\n",
       " 'said': 360,\n",
       " 'sam': 361,\n",
       " 'saw': 362,\n",
       " 'say': 363,\n",
       " 'scare': 364,\n",
       " 'sceneri': 365,\n",
       " 'school': 366,\n",
       " 'score': 367,\n",
       " 'screen': 368,\n",
       " 'shatter': 369,\n",
       " 'silenc': 370,\n",
       " 'simpl': 371,\n",
       " 'skywalk': 372,\n",
       " 'slight': 373,\n",
       " 'slow': 374,\n",
       " 'somehow': 375,\n",
       " 'soon': 376,\n",
       " 'sort': 377,\n",
       " 'sound': 378,\n",
       " 'special': 379,\n",
       " 'statement': 380,\n",
       " 'stop': 381,\n",
       " 'strong': 382,\n",
       " 'summari': 383,\n",
       " 'suppos': 384,\n",
       " 'tast': 385,\n",
       " 'ten': 386,\n",
       " 'thoma': 387,\n",
       " 'three': 388,\n",
       " 'titl': 389,\n",
       " 'togeth': 390,\n",
       " 'touch': 391,\n",
       " 'track': 392,\n",
       " 'tree': 393,\n",
       " 'tv': 394,\n",
       " 'uniqu': 395,\n",
       " 'univers': 396,\n",
       " 'unlik': 397,\n",
       " 'use': 398,\n",
       " 'version': 399,\n",
       " 'view': 400,\n",
       " 'villag': 401,\n",
       " 'winter': 402,\n",
       " 'word': 403,\n",
       " 'world': 404,\n",
       " 'write': 405,\n",
       " 'wrong': 406,\n",
       " '09': 407,\n",
       " '12': 408,\n",
       " '17': 409,\n",
       " '1930s': 410,\n",
       " '1950s': 411,\n",
       " '2005': 412,\n",
       " '2nd': 413,\n",
       " '80': 414,\n",
       " '9': 415,\n",
       " '99': 416,\n",
       " 'abduct': 417,\n",
       " 'abject': 418,\n",
       " 'abov': 419,\n",
       " 'abroad': 420,\n",
       " 'absolut': 421,\n",
       " 'academi': 422,\n",
       " 'access': 423,\n",
       " 'accid': 424,\n",
       " 'accident': 425,\n",
       " 'ador': 426,\n",
       " 'advic': 427,\n",
       " 'affirm': 428,\n",
       " 'age': 429,\n",
       " 'ahead': 430,\n",
       " 'air': 431,\n",
       " 'akshay': 432,\n",
       " 'alittl': 433,\n",
       " 'aliv': 434,\n",
       " 'along': 435,\n",
       " 'alreadi': 436,\n",
       " 'altern': 437,\n",
       " 'amaz': 438,\n",
       " 'among': 439,\n",
       " 'angri': 440,\n",
       " 'anim': 441,\n",
       " 'annoy': 442,\n",
       " 'answer': 443,\n",
       " 'anthropophagus': 444,\n",
       " 'anymor': 445,\n",
       " 'anyon': 446,\n",
       " 'anyth': 447,\n",
       " 'appeal': 448,\n",
       " 'appreci': 449,\n",
       " 'april': 450,\n",
       " 'around': 451,\n",
       " 'arthous': 452,\n",
       " 'arthrit': 453,\n",
       " 'artist': 454,\n",
       " 'artless': 455,\n",
       " 'aspect': 456,\n",
       " 'ass': 457,\n",
       " 'atmospher': 458,\n",
       " 'attempt': 459,\n",
       " 'attir': 460,\n",
       " 'avenu': 461,\n",
       " 'avow': 462,\n",
       " 'axe': 463,\n",
       " 'bag': 464,\n",
       " 'banal': 465,\n",
       " 'bar': 466,\n",
       " 'barr': 467,\n",
       " 'barri': 468,\n",
       " 'base': 469,\n",
       " 'befor': 470,\n",
       " 'behav': 471,\n",
       " 'behind': 472,\n",
       " 'bernard': 473,\n",
       " 'biggest': 474,\n",
       " 'birthday': 475,\n",
       " 'black': 476,\n",
       " 'bleak': 477,\n",
       " 'blew': 478,\n",
       " 'blucher': 479,\n",
       " 'boil': 480,\n",
       " 'bond': 481,\n",
       " 'boob': 482,\n",
       " 'boon': 483,\n",
       " 'border': 484,\n",
       " 'boss': 485,\n",
       " 'bostwick': 486,\n",
       " 'bought': 487,\n",
       " 'box': 488,\n",
       " 'brave': 489,\n",
       " 'breath': 490,\n",
       " 'brenda': 491,\n",
       " 'britney': 492,\n",
       " 'brow': 493,\n",
       " 'bruce': 494,\n",
       " 'budget': 495,\n",
       " 'build': 496,\n",
       " 'built': 497,\n",
       " 'bunch': 498,\n",
       " 'california': 499,\n",
       " 'calm': 500,\n",
       " 'cameo': 501,\n",
       " 'cant': 502,\n",
       " 'car': 503,\n",
       " 'carri': 504,\n",
       " 'case': 505,\n",
       " 'cash': 506,\n",
       " 'cat': 507,\n",
       " 'catalogu': 508,\n",
       " 'catch': 509,\n",
       " 'cathol': 510,\n",
       " 'caught': 511,\n",
       " 'caus': 512,\n",
       " 'cave': 513,\n",
       " 'censor': 514,\n",
       " 'centerlin': 515,\n",
       " 'cerebr': 516,\n",
       " 'certain': 517,\n",
       " 'chang': 518,\n",
       " 'charg': 519,\n",
       " 'charl': 520,\n",
       " 'charlott': 521,\n",
       " 'cheat': 522,\n",
       " 'cheer': 523,\n",
       " 'chemic': 524,\n",
       " 'childhood': 525,\n",
       " 'children': 526,\n",
       " 'choic': 527,\n",
       " 'chooper': 528,\n",
       " 'chose': 529,\n",
       " 'christma': 530,\n",
       " 'christmas': 531,\n",
       " 'church': 532,\n",
       " 'cinema': 533,\n",
       " 'cinephil': 534,\n",
       " 'clean': 535,\n",
       " 'clever': 536,\n",
       " 'clich': 537,\n",
       " 'clichd': 538,\n",
       " 'climax': 539,\n",
       " 'close': 540,\n",
       " 'closet': 541,\n",
       " 'collect': 542,\n",
       " 'colleg': 543,\n",
       " 'color': 544,\n",
       " 'colour': 545,\n",
       " 'comment': 546,\n",
       " 'commit': 547,\n",
       " 'compass': 548,\n",
       " 'compassion': 549,\n",
       " 'compromis': 550,\n",
       " 'concept': 551,\n",
       " 'conduct': 552,\n",
       " 'confid': 553,\n",
       " 'congest': 554,\n",
       " 'conner': 555,\n",
       " 'conserv': 556,\n",
       " 'consist': 557,\n",
       " 'contain': 558,\n",
       " 'continu': 559,\n",
       " 'contrast': 560,\n",
       " 'convent': 561,\n",
       " 'convinc': 562,\n",
       " 'cook': 563,\n",
       " 'cop': 564,\n",
       " 'countri': 565,\n",
       " 'courag': 566,\n",
       " 'crane': 567,\n",
       " 'creation': 568,\n",
       " 'credenti': 569,\n",
       " 'crew': 570,\n",
       " 'crime': 571,\n",
       " 'critic': 572,\n",
       " 'crucial': 573,\n",
       " 'cthd': 574,\n",
       " 'cut': 575,\n",
       " 'cute': 576,\n",
       " 'dad': 577,\n",
       " 'damn': 578,\n",
       " 'daniel': 579,\n",
       " 'dark': 580,\n",
       " 'darshan': 581,\n",
       " 'daunt': 582,\n",
       " 'davenport': 583,\n",
       " 'deal': 584,\n",
       " 'decad': 585,\n",
       " 'defend': 586,\n",
       " 'defin': 587,\n",
       " 'definit': 588,\n",
       " 'delic': 589,\n",
       " 'deliri': 590,\n",
       " 'demeanor': 591,\n",
       " 'depart': 592,\n",
       " 'desai': 593,\n",
       " 'descent': 594,\n",
       " 'desir': 595,\n",
       " 'detail': 596,\n",
       " 'dialog': 597,\n",
       " 'diarrhea': 598,\n",
       " 'differ': 599,\n",
       " 'dim': 600,\n",
       " 'dimens': 601,\n",
       " 'disappear': 602,\n",
       " 'divin': 603,\n",
       " 'dollar': 604,\n",
       " 'donovan': 605,\n",
       " 'dorki': 606,\n",
       " 'dorm': 607,\n",
       " 'drawn': 608,\n",
       " 'drew': 609,\n",
       " 'driven': 610,\n",
       " 'drop': 611,\n",
       " 'drunk': 612,\n",
       " 'dub': 613,\n",
       " 'dubai': 614,\n",
       " 'dull': 615,\n",
       " 'dure': 616,\n",
       " 'dysfunct': 617,\n",
       " 'dysfunctin': 618,\n",
       " 'e': 619,\n",
       " 'earlier': 620,\n",
       " 'eas': 621,\n",
       " 'easi': 622,\n",
       " 'eat': 623,\n",
       " 'edit': 624,\n",
       " 'effect': 625,\n",
       " 'eight': 626,\n",
       " 'either': 627,\n",
       " 'elast': 628,\n",
       " 'eldest': 629,\n",
       " 'embarrass': 630,\n",
       " 'emma': 631,\n",
       " 'emot': 632,\n",
       " 'encourag': 633,\n",
       " 'english': 634,\n",
       " 'epic': 635,\n",
       " 'equal': 636,\n",
       " 'erupt': 637,\n",
       " 'estim': 638,\n",
       " 'estrang': 639,\n",
       " 'etc': 640,\n",
       " 'event': 641,\n",
       " 'eventu': 642,\n",
       " 'everyon': 643,\n",
       " 'ewok': 644,\n",
       " 'excess': 645,\n",
       " 'exit': 646,\n",
       " 'experienc': 647,\n",
       " 'explan': 648,\n",
       " 'expos': 649,\n",
       " 'express': 650,\n",
       " 'extra': 651,\n",
       " 'exuber': 652,\n",
       " 'eye': 653,\n",
       " 'f': 654,\n",
       " 'fab': 655,\n",
       " 'face': 656,\n",
       " 'faci': 657,\n",
       " 'factor': 658,\n",
       " 'fail': 659,\n",
       " 'faith': 660,\n",
       " 'fart': 661,\n",
       " 'fast': 662,\n",
       " 'fatuous': 663,\n",
       " 'fe': 664,\n",
       " 'fear': 665,\n",
       " 'felt': 666,\n",
       " 'festiv': 667,\n",
       " 'fetid': 668,\n",
       " 'fiction': 669,\n",
       " 'filmmak': 670,\n",
       " 'fish': 671,\n",
       " 'five': 672,\n",
       " 'flashback': 673,\n",
       " 'fli': 674,\n",
       " 'flock': 675,\n",
       " 'floor': 676,\n",
       " 'focus': 677,\n",
       " 'folk': 678,\n",
       " 'fond': 679,\n",
       " 'forc': 680,\n",
       " 'foreshadow': 681,\n",
       " 'form': 682,\n",
       " 'former': 683,\n",
       " 'fort': 684,\n",
       " 'forum': 685,\n",
       " 'foul': 686,\n",
       " 'found': 687,\n",
       " 'foundat': 688,\n",
       " 'four': 689,\n",
       " 'fragil': 690,\n",
       " 'frau': 691,\n",
       " 'freedom': 692,\n",
       " 'freeway': 693,\n",
       " 'freudian': 694,\n",
       " 'fricker': 695,\n",
       " 'frustrat': 696,\n",
       " 'fulfil': 697,\n",
       " 'fun': 698,\n",
       " 'furious': 699,\n",
       " 'gage': 700,\n",
       " 'galaxi': 701,\n",
       " 'gang': 702,\n",
       " 'gangster': 703,\n",
       " 'garbag': 704,\n",
       " 'general': 705,\n",
       " 'generat': 706,\n",
       " 'genr': 707,\n",
       " 'gentl': 708,\n",
       " 'gibson': 709,\n",
       " 'given': 710,\n",
       " 'glib': 711,\n",
       " 'glorious': 712,\n",
       " 'goal': 713,\n",
       " 'goblet': 714,\n",
       " 'goe': 715,\n",
       " 'goer': 716,\n",
       " 'goofi': 717,\n",
       " 'gori': 718,\n",
       " 'grand': 719,\n",
       " 'grasp': 720,\n",
       " 'grow': 721,\n",
       " 'guess': 722,\n",
       " 'guitar': 723,\n",
       " 'guru': 724,\n",
       " 'guruk': 725,\n",
       " 'hall': 726,\n",
       " 'hamwork': 727,\n",
       " 'han': 728,\n",
       " 'hank': 729,\n",
       " 'hard': 730,\n",
       " 'haril': 731,\n",
       " 'harlen': 732,\n",
       " 'harmless': 733,\n",
       " 'harsh': 734,\n",
       " 'hate': 735,\n",
       " 'hawk': 736,\n",
       " 'heartstr': 737,\n",
       " 'height': 738,\n",
       " 'hell': 739,\n",
       " 'helmet': 740,\n",
       " 'heorot': 741,\n",
       " 'herd': 742,\n",
       " 'heroin': 743,\n",
       " 'hilari': 744,\n",
       " 'hilton': 745,\n",
       " 'hipper': 746,\n",
       " 'hit': 747,\n",
       " 'hitchcock': 748,\n",
       " 'hjm': 749,\n",
       " 'home': 750,\n",
       " 'horn': 751,\n",
       " 'hoth': 752,\n",
       " 'hour': 753,\n",
       " 'huge': 754,\n",
       " 'hugh': 755,\n",
       " 'humbl': 756,\n",
       " 'hunt': 757,\n",
       " 'hype': 758,\n",
       " 'identifi': 759,\n",
       " 'ideolog': 760,\n",
       " 'ididn': 761,\n",
       " 'ignor': 762,\n",
       " 'ilk': 763,\n",
       " 'illinoi': 764,\n",
       " 'impal': 765,\n",
       " 'impecc': 766,\n",
       " 'import': 767,\n",
       " 'inanim': 768,\n",
       " 'inbetween': 769,\n",
       " 'includ': 770,\n",
       " 'india': 771,\n",
       " 'indian': 772,\n",
       " 'industri': 773,\n",
       " 'inept': 774,\n",
       " 'insight': 775,\n",
       " 'insinu': 776,\n",
       " 'instanc': 777,\n",
       " 'instinct': 778,\n",
       " 'insult': 779,\n",
       " 'intellig': 780,\n",
       " 'intensifi': 781,\n",
       " 'intent': 782,\n",
       " 'intervent': 783,\n",
       " 'introduct': 784,\n",
       " 'investig': 785,\n",
       " 'involv': 786,\n",
       " 'jargon': 787,\n",
       " 'jariwala': 788,\n",
       " 'jason': 789,\n",
       " 'jean': 790,\n",
       " 'jim': 791,\n",
       " 'joan': 792,\n",
       " 'jog': 793,\n",
       " 'jone': 794,\n",
       " 'joyous': 795,\n",
       " 'jude': 796,\n",
       " 'julia': 797,\n",
       " 'justic': 798,\n",
       " 'k': 799,\n",
       " 'kansa': 800,\n",
       " 'karamchand': 801,\n",
       " 'kasturba': 802,\n",
       " 'khanna': 803,\n",
       " 'kick': 804,\n",
       " 'kill': 805,\n",
       " 'kingsley': 806,\n",
       " 'knee': 807,\n",
       " 'knew': 808,\n",
       " 'knockoff': 809,\n",
       " 'knuckl': 810,\n",
       " 'lack': 811,\n",
       " 'laden': 812,\n",
       " 'lake': 813,\n",
       " 'lambert': 814,\n",
       " 'land': 815,\n",
       " 'landscap': 816,\n",
       " 'later': 817,\n",
       " 'laugh': 818,\n",
       " 'law': 819,\n",
       " 'led': 820,\n",
       " 'lee': 821,\n",
       " 'leg': 822,\n",
       " 'legendari': 823,\n",
       " 'leigh': 824,\n",
       " 'less': 825,\n",
       " 'lie': 826,\n",
       " 'liebman': 827,\n",
       " 'likabl': 828,\n",
       " 'lindsay': 829,\n",
       " 'literari': 830,\n",
       " 'literatur': 831,\n",
       " 'littl': 832,\n",
       " 'lohan': 833,\n",
       " 'longer': 834,\n",
       " 'lookout': 835,\n",
       " 'lord': 836,\n",
       " 'lotr': 837,\n",
       " 'loud': 838,\n",
       " 'luca': 839,\n",
       " 'lunat': 840,\n",
       " 'maclean': 841,\n",
       " 'magazin': 842,\n",
       " 'magic': 843,\n",
       " 'maguir': 844,\n",
       " 'mail': 845,\n",
       " 'maintain': 846,\n",
       " 'mall': 847,\n",
       " 'manag': 848,\n",
       " 'mari': 849,\n",
       " 'marlow': 850,\n",
       " 'marriag': 851,\n",
       " 'match': 852,\n",
       " 'materialist': 853,\n",
       " 'maul': 854,\n",
       " 'mayb': 855,\n",
       " 'mcanal': 856,\n",
       " 'meant': 857,\n",
       " 'media': 858,\n",
       " 'mediocr': 859,\n",
       " 'meeker': 860,\n",
       " 'mel': 861,\n",
       " 'mellow': 862,\n",
       " 'men': 863,\n",
       " 'mention': 864,\n",
       " 'mercedez': 865,\n",
       " 'merit': 866,\n",
       " 'mile': 867,\n",
       " 'misdirect': 868,\n",
       " 'miseri': 869,\n",
       " 'mix': 870,\n",
       " 'mode': 871,\n",
       " 'model': 872,\n",
       " 'mohanda': 873,\n",
       " 'money': 874,\n",
       " 'monica': 875,\n",
       " 'monsoon': 876,\n",
       " 'montana': 877,\n",
       " 'moscovit': 878,\n",
       " 'mother': 879,\n",
       " 'moulin': 880,\n",
       " 'mountain': 881,\n",
       " 'mountainsid': 882,\n",
       " 'movement': 883,\n",
       " 'murder': 884,\n",
       " 'mysteri': 885,\n",
       " 'mystic': 886,\n",
       " 'natur': 887,\n",
       " 'near': 888,\n",
       " 'nevermind': 889,\n",
       " 'nicola': 890,\n",
       " 'night': 891,\n",
       " 'nightmar': 892,\n",
       " 'niko': 893,\n",
       " 'northern': 894,\n",
       " 'northwest': 895,\n",
       " 'notabl': 896,\n",
       " 'note': 897,\n",
       " 'novel': 898,\n",
       " 'novella': 899,\n",
       " 'nugget': 900,\n",
       " 'number': 901,\n",
       " 'nun': 902,\n",
       " 'nut': 903,\n",
       " 'object': 904,\n",
       " 'occur': 905,\n",
       " 'often': 906,\n",
       " 'oh': 907,\n",
       " 'opposit': 908,\n",
       " 'order': 909,\n",
       " 'otherwis': 910,\n",
       " 'outlook': 911,\n",
       " 'outstand': 912,\n",
       " 'overlook': 913,\n",
       " 'overplay': 914,\n",
       " 'overwrought': 915,\n",
       " 'paint': 916,\n",
       " 'palsi': 917,\n",
       " 'pantheon': 918,\n",
       " 'pari': 919,\n",
       " 'park': 920,\n",
       " 'parker': 921,\n",
       " 'pass': 922,\n",
       " 'passion': 923,\n",
       " 'pat': 924,\n",
       " 'path': 925,\n",
       " 'pathet': 926,\n",
       " 'patho': 927,\n",
       " 'paul': 928,\n",
       " 'paus': 929,\n",
       " 'payoff': 930,\n",
       " 'peak': 931,\n",
       " 'peev': 932,\n",
       " 'perhap': 933,\n",
       " 'period': 934,\n",
       " 'perspect': 935,\n",
       " 'pick': 936,\n",
       " 'pictur': 937,\n",
       " 'piec': 938,\n",
       " 'pin': 939,\n",
       " 'pine': 940,\n",
       " 'piti': 941,\n",
       " 'place': 942,\n",
       " 'placement': 943,\n",
       " 'plain': 944,\n",
       " 'plan': 945,\n",
       " 'player': 946,\n",
       " 'playwright': 947,\n",
       " 'pollut': 948,\n",
       " 'poor': 949,\n",
       " 'portray': 950,\n",
       " 'poss': 951,\n",
       " 'possibl': 952,\n",
       " 'postcard': 953,\n",
       " 'practic': 954,\n",
       " 'preced': 955,\n",
       " 'predict': 956,\n",
       " 'present': 957,\n",
       " 'preserv': 958,\n",
       " 'pretti': 959,\n",
       " 'prevent': 960,\n",
       " 'priest': 961,\n",
       " 'prima': 962,\n",
       " 'process': 963,\n",
       " 'produc': 964,\n",
       " 'product': 965,\n",
       " 'profan': 966,\n",
       " 'profit': 967,\n",
       " 'program': 968,\n",
       " 'progress': 969,\n",
       " 'prolong': 970,\n",
       " 'prom': 971,\n",
       " 'promin': 972,\n",
       " 'proof': 973,\n",
       " 'protect': 974,\n",
       " 'prove': 975,\n",
       " 'psycho': 976,\n",
       " 'psychoanalyst': 977,\n",
       " 'psychosi': 978,\n",
       " 'purchas': 979,\n",
       " 'qauntiti': 980,\n",
       " 'question': 981,\n",
       " 'rachel': 982,\n",
       " 'racial': 983,\n",
       " 'raghava': 984,\n",
       " 'raghupati': 985,\n",
       " 'ralph': 986,\n",
       " 'rare': 987,\n",
       " 'rariti': 988,\n",
       " 'rate': 989,\n",
       " 'rationalist': 990,\n",
       " 'ray': 991,\n",
       " 'realm': 992,\n",
       " 'receiv': 993,\n",
       " 'recommend': 994,\n",
       " 'redempt': 995,\n",
       " 'redneck': 996,\n",
       " 'refresh': 997,\n",
       " 'relax': 998,\n",
       " 'reliev': 999,\n",
       " 'remak': 1000,\n",
       " 'remind': 1001,\n",
       " 'repres': 1002,\n",
       " ...}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce.word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['more' '11443']\n",
      " ['she' '11308']\n",
      " ['when' '11303']\n",
      " ...\n",
      " ['dud' '66']\n",
      " ['enormous' '66']\n",
      " ['examination' '66']]\n"
     ]
    }
   ],
   "source": [
    "word_to_index, index_to_word, X_train_prep = wordcount_encoding(X_train, 50, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_prep = []\n",
    "#for doc in X_test:\n",
    "#    doc_prep = []\n",
    "#    for word in doc:\n",
    "#        try:\n",
    "#            doc_prep.append(word_to_index[word])\n",
    "#        except:\n",
    "#            doc_prep.append(1)\n",
    "#    X_test_prep.append(doc_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20000.000000\n",
       "mean       114.196600\n",
       "std         83.682466\n",
       "min          3.000000\n",
       "25%         63.000000\n",
       "50%         86.000000\n",
       "75%        139.000000\n",
       "max       1118.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(X_train_prep).apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train_prep, maxlen=150)\n",
    "X_test_pad = pad_sequences(X_test_prep, maxlen=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    1, 1003,  913,  924, 2818, 1759, 3977, 1298, 3233, 1555,\n",
       "        1759, 3977, 1167, 1156,  232,  963,  220, 1224,  243, 3312,  668,\n",
       "        3977, 1962,  135,  931,   26,  840,  452, 3977,   34, 3464,   38,\n",
       "          28,  423,   22,   49,  913,   26,  195,  181,  789, 1227, 3690,\n",
       "        1175,   31,  492,  156,  242, 4744,  248,  218,  776, 1809,   28,\n",
       "         173, 2487, 1305,   96, 2297, 1962,  572,  132, 1226,    5,  319,\n",
       "        2586, 2317, 2944,    4,   47,   71,  448,    5,  319, 2580,   96,\n",
       "         276,  196,  951,  144, 3690, 1175,   28,  173, 3977, 3850, 1509,\n",
       "          51,  215,  547,  485,  238, 1453,  183,  451,  215,  173, 1240,\n",
       "          23,  172,  319, 2362, 1479, 1635, 1926,   28,  157,   27,   59,\n",
       "          13,   14,  229,  166,   57,  690,  110],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    1,  136, 1456,   75,\n",
       "        1466, 2489,   18,  120,   88,   99,  102, 1198,   86,  133, 1034,\n",
       "         439,   43, 1985,  116, 1466,  590,    8,   97,  712,  395,   80,\n",
       "          14,  103,   42,  236, 4337, 2820,  395,    6,   80,  482,   13,\n",
       "          12,  306, 1182,  399,   10,   20,   65,   17,  722, 1443,  237,\n",
       "         702,  455,    9,   26,  495,  389,  103,   72,  164,  184,  232,\n",
       "          15,  114,  515,  170,   41,  515,  726,  325,   33,   30,  399,\n",
       "          10,  174,  363, 4414,    5, 1046, 1011]], dtype=int32)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Dropout\n",
    "top_words = np.matrix(X_train_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train_prep = []\n",
    "len(X_train_pad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4952"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_pad).max().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_27 (Embedding)     (None, None, 256)         1268224   \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 150)               244200    \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 1,512,575\n",
      "Trainable params: 1,512,575\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 32 \n",
    "model = Sequential() \n",
    "model.add(Embedding(4952+2, 256))\n",
    "model.add(LSTM(150,dropout=0.4, recurrent_dropout=0.4)) \n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy']) \n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jodahr/anaconda3/envs/TextMining/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 85s 5ms/step - loss: 0.4933 - acc: 0.7627 - val_loss: 0.3510 - val_acc: 0.8552\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 80s 5ms/step - loss: 0.3351 - acc: 0.8643 - val_loss: 0.3737 - val_acc: 0.8367\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 80s 5ms/step - loss: 0.2803 - acc: 0.8901 - val_loss: 0.3516 - val_acc: 0.8522\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 78s 5ms/step - loss: 0.2477 - acc: 0.9058 - val_loss: 0.3838 - val_acc: 0.8425\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 78s 5ms/step - loss: 0.2156 - acc: 0.9191 - val_loss: 0.3885 - val_acc: 0.8500\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 78s 5ms/step - loss: 0.1886 - acc: 0.9296 - val_loss: 0.3889 - val_acc: 0.8615\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 80s 5ms/step - loss: 0.1674 - acc: 0.9386 - val_loss: 0.4182 - val_acc: 0.8445\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 81s 5ms/step - loss: 0.1478 - acc: 0.9444 - val_loss: 0.4547 - val_acc: 0.8588\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 80s 5ms/step - loss: 0.1392 - acc: 0.9487 - val_loss: 0.4817 - val_acc: 0.8512\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 83s 5ms/step - loss: 0.1243 - acc: 0.9544 - val_loss: 0.5170 - val_acc: 0.8275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4866ef8518>"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_pad, y_train, validation_split=0.2, nb_epoch=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('was', 4), ('geht', 2), ('ab', 2), ('hier', 6), ('soll', 2), ('das', 2), ('ist', 2), ('es', 2), ('langweilig', 2)])\n",
      "6\n",
      "{'hier': 1, 'was': 2, 'geht': 3, 'ab': 4, 'soll': 5, 'das': 6, 'ist': 7, 'es': 8, 'langweilig': 9}\n",
      "{'hier': 6, 'was': 4, 'geht': 2, 'ab': 2, 'das': 2, 'soll': 2, 'ist': 2, 'langweilig': 2, 'es': 2}\n"
     ]
    }
   ],
   "source": [
    "tk.fit_on_texts(corpus)\n",
    "print(tk.word_counts)\n",
    "print(tk.document_count)\n",
    "print(tk.word_index)\n",
    "print(tk.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "encoded_docs = tk.texts_to_matrix(corpus, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([' ', '?', 'H', 'W', 'a', 'b', 'd', 'e', 'g', 'h', 'i', 'l', 'n',\n",
       "        'o', 'r', 's', 't', 'w'], dtype='<U1'),\n",
       " array([9, 1, 1, 2, 5, 1, 1, 6, 3, 3, 6, 4, 1, 1, 3, 6, 2, 1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.wordcount_corpus(corpus,flatten=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('reviews.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 5 columns):\n",
      "id                               25000 non-null object\n",
      "sentiment                        25000 non-null int64\n",
      "review                           25000 non-null object\n",
      "review_preprocessed              25000 non-null object\n",
      "review_preprocessed_tokenized    25000 non-null object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 976.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['review_preprocessed_tokenized'],\n",
    "                                                    df['sentiment'], test_size=0.2, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23311    [movi, plain, dumb, cast, ralph, meeker, mike,...\n",
       "23623    [dahmer, young, confus, man, dahmer, confus, m...\n",
       "1020     [may, saint, preserv, us, becaus, movi, go, he...\n",
       "12645    [combin, read, novella, view, film, inspir, wi...\n",
       "1533     [daniel, day, lewi, left, foot, give, us, one,...\n",
       "3518     [perhap, former, moscovit, elast, sens, humor,...\n",
       "483      [kid, took, movi, love, four, children, age, 9...\n",
       "19370    [well, well, roeg, touch, bit, nerv, genius, c...\n",
       "12667    [excel, film, understand, whi, mani, peopl, li...\n",
       "7403     [particular, fond, remak, steal, modern, jargo...\n",
       "2712     [think, movi, got, low, rate, becaus, got, jud...\n",
       "11605    [read, plot, summari, worst, one, ever, read, ...\n",
       "7921     [wait, wait, film, come, trailer, seem, year, ...\n",
       "7801     [premis, may, seem, goofi, sinc, murphi, chara...\n",
       "24811    [back, high, school, day, salina, kansa, film,...\n",
       "20273    [act, film, veri, well, act, film, say, perfor...\n",
       "6750     [movi, classic, perfect, certain, pace, perfec...\n",
       "19637    [long, time, ago, galaxi, far, far, away, boy,...\n",
       "23533    [god, name, happen, doe, one, go, creat, pract...\n",
       "16623    [movi, dysfunctin, famili, ani, dysfunct, fami...\n",
       "10425    [horribl, see, mani, user, comment, great, sho...\n",
       "14642    [one, favorit, movi, time, great, act, brillia...\n",
       "14443    [70s, charlton, heston, star, sci, fi, flick, ...\n",
       "24472    [befor, start, let, say, fulli, believ, god, b...\n",
       "11895    [wow, love, movi, normal, life, small, villag,...\n",
       "11535    [first, half, ok, last, half, realli, realli, ...\n",
       "22951    [thorough, diabol, tale, bad, thing, go, wrong...\n",
       "3947     [realli, zombi, film, defin, zombi, dead, walk...\n",
       "18310    [teenag, pretti, whole, bigfoot, thing, read, ...\n",
       "3284     [went, movi, cast, crew, show, caus, friend, p...\n",
       "                               ...                        \n",
       "24233    [take, serbian, least, balkan, familiar, under...\n",
       "18942    [well, suppos, good, news, concern, william, w...\n",
       "8666     [oscar, nomin, turn, secret, lie, brenda, blet...\n",
       "6396     [find, rather, useless, comment, movi, simples...\n",
       "19769    [yakit, ja, pan, translat, fresh, bake, japane...\n",
       "20939    [xizao, tale, clash, modern, life, ancient, tr...\n",
       "17568    [review, base, dub, shock, rama, video, releas...\n",
       "6420     [spot, noah, wyle, ricki, schroder, movi, like...\n",
       "5051     [hilari, averag, schmo, lead, pathet, life, su...\n",
       "5311     [detect, toni, rome, frank, sinatra, return, s...\n",
       "2433     [veri, first, episod, friend, air, 22, sept, 1...\n",
       "23333    [line, work, occasion, get, contact, independ,...\n",
       "769      [recent, purchas, collect, one, awesom, seri, ...\n",
       "1685     [screen, write, dumb, pain, wast, 2, hour, lif...\n",
       "8322     [mardi, gras, made, china, excel, movi, depict...\n",
       "16023    [great, stori, base, true, stori, young, black...\n",
       "11363    [admir, odd, though, mean, spirit, comedi, dra...\n",
       "14423    [staci, peralta, true, first, peopl, live, sto...\n",
       "21962    [enjoy, lot, watch, movi, great, direct, alrea...\n",
       "4426     [jane, eyr, one, greatest, novel, english, lan...\n",
       "16850    [first, although, mani, say, worst, seri, thin...\n",
       "6265     [joyous, world, creat, us, pixar, bug, life, i...\n",
       "22118    [dakota, 1988, anoth, earli, lou, diamond, phi...\n",
       "11284    [shadrach, favorit, type, movi, found, sentime...\n",
       "11964    [friend, mine, gave, movi, friend, mine, hospi...\n",
       "21575    [distast, clich, thriller, young, coupl, cross...\n",
       "5390     [stewart, wyom, cattleman, dream, make, enough...\n",
       "860      [1928, mani, way, lost, year, motion, pictur, ...\n",
       "15795    [felt, film, throughout, waa, impress, russel,...\n",
       "23654    [intent, director, film, quit, honor, histori,...\n",
       "Name: review_preprocessed_tokenized, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jodahr/jupyter/notebooks/Misc/MovieRatings-TextMining\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word2index, indextword = utils.get_vocabulary(df.review_preprocessed_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_index(token_list, word2index):\n",
    "    index_sequence = []\n",
    "    for element in token_list:\n",
    "        index = word2index[element]\n",
    "        index_sequence.append(index)\n",
    "    return index_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_index'] = df.review_preprocessed_tokenized.apply(lambda x: sentence_to_index(x,word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 6 columns):\n",
      "id                               25000 non-null object\n",
      "sentiment                        25000 non-null int64\n",
      "review                           25000 non-null object\n",
      "review_preprocessed              25000 non-null object\n",
      "review_preprocessed_tokenized    25000 non-null object\n",
      "review_index                     25000 non-null object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['doc_length'] = df.review_index.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['doc_length'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (TextMining)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
